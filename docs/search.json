[
  {
    "objectID": "supplementary/location-scale-models.html",
    "href": "supplementary/location-scale-models.html",
    "title": "Location-Scale Ordinal Models",
    "section": "",
    "text": "The default ordinal regression model assume that the variance of the underlying latent distribution is the same across conditions. This is similar to a standard linear regression assuming the homogeneity of variances.\nFor example, when comparing two groups or conditions we can run a standard linear model (i.e., a t-test) assuming homogeneity of variances or using the Welch t-test (see Delacre et al., 2017) that relaxes the assumption. In addition, there are the so-called location-scale models that allows to include predictors also for the scale (e.g., the variance) of the distribution.\nThis can be done also in ordinal regression where instead of assuming the same variance between conditions, the linear predictors can be included. The Equations 1 and 2 expand the standard model including the linear predictor on the scale of the latent distribution.\n\\[\nP(Y \\leq k) = g^{-1}\\left(\\frac{\\alpha_k - \\mathbf{X}\\boldsymbol{\\beta}}{e^{\\boldsymbol{Z}\\boldsymbol{\\zeta}}}\\right)\n\\tag{1}\\]\n\\[\nY^\\star = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\;\\;\\; \\epsilon_i \\sim \\mathcal{N}(0, e^{Z\\zeta})\n\\tag{2}\\]\nWhere \\(X\\zeta\\) is the linear predictor for the scale of the distribution. By default for both the logit and probit model the scale is fixed to 1. On scale-location models we put predictors on both parameters. Given that the scale cannot be negative we use a log link function \\(\\eta = \\text{log}(X\\zeta)\\). As suggested by Tutz & Berger (2017) location-scale models can be considered as a more parsimonious approach compared to partially or completely relaxing the proportional odds assumption. Allowing the scale to be different as a function of the predictor create more modelling flexibility. Furthermore, two groups could be theoretically different only in the scale of the latent distribution with a similar location. In this example, the only way to capture group differences is by including a scale effect. The Figure 1 depict the impact of having different scales between two groups on the ordinal probabilities but the same location. Clearly, there is no location effect and the two groups are predicted to be the same considering only the location effect.\n\ncat_latent_plot(location = c(0, 0), scale = c(1, 2), prob0 = rep(1/4, 4), link = \"logit\", plot = FALSE) |&gt; \n  cowplot::plot_grid(plotlist = _, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nTutz (2022) provide a very clear and intuitive explanation of what happen when including a scale effect and how to interpret the result. As suggested before, the scale-location model allow to independently predict changes in the location and the scale. While location shifts are simply interpreted as increasing/decreasing the latent \\(\\mu\\) or the odds of responding a certain category scale effects are not straightforward. As the scale increase (e.g., the variance increase) there is an higher probability mass on extreme categories. On the other side as the scale decrease, responses are more concentrated on specific categories.\nThe location parameter determine the category and the scale determine the concentration around the category. For example, if one group have a certain latent mean \\(\\mu_1\\) and a small scale \\(\\sigma^2 = 1/3\\) (thus one third compared to the standard version of the distribution), all responses will be focused on categories around the latent mean. On the other side, increasing the scale will increase the cumulative probabilities for all categories and for values that tends to infinity extreme categories are preferred. The scale parameter can somehow be interpreted as the response style.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe location-scale model can be simulated using the sim_ord_latent() function and providing the predictors for the scale parameters. Given the log link function, predictors are provided on the log scale. For example, we simulate the effect of a binary variable \\(x\\) representing two independent groups predicting the \\(k = 5\\) response. We simulate a location effect of \\(\\beta_1 = 0.5\\) (in probit scale) and \\(\\zeta_1 = \\text{log}(2) = 0.70\\). The first group has a \\(\\sigma = 1\\) and the second group has \\(\\sigma = 2\\). Again we simulate that the baseline probabilities are uniform for the first group. The \\(\\log\\) link function (and \\(e\\) as inverse link function) is used to make sure that the variance is always positive. If the first group has a scale of 1 and the second group a scale of 2, the \\(\\zeta_1 = \\log(\\frac{s_2}{s_1}) = \\log(\\frac{2}{1}) \\approx 0.7\\). Thus the two groups have a log difference in terms of scale of 0.7 (or a ratio of 2).\n\nk &lt;- 5  # number of options\nn &lt;- 1e5 # number of observations\nb1 &lt;- 0.5 # beta1, the shift in the latent distribution\nz1 &lt;- log(2) # zeta1, the change in the scale\nprobs0 &lt;- rep(1/k, k) # probabilities when x = 0\nalphas &lt;- prob_to_alpha(probs0, link = \"probit\") # get true thresholds from probabilities\ndat &lt;- data.frame(x = rep(c(0, 1), each = n/2))\ndat &lt;- sim_ord_latent(~x, scale = ~x, beta = b1, zeta = z1, prob0 = probs0, data = dat, link = \"probit\")\nfit &lt;- clm(y ~ x, scale = ~ x, data = dat, link = \"probit\")\nsummary(fit)\n\nformula: y ~ x\nscale:   ~x\ndata:    dat\n\n link   threshold nobs  logLik     AIC       niter max.grad cond.H \n probit flexible  1e+05 -151495.53 303003.05 11(0) 4.16e-07 3.1e+01\n\nCoefficients:\n  Estimate Std. Error z value Pr(&gt;|z|)    \nx  0.48797    0.01165   41.88   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlog-scale coefficients:\n  Estimate Std. Error z value Pr(&gt;|z|)    \nx 0.694460   0.008343   83.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n     Estimate Std. Error z value\n1|2 -0.849586   0.006316  -134.5\n2|3 -0.258767   0.005425   -47.7\n3|4  0.247654   0.005407    45.8\n4|5  0.832103   0.006281   132.5\n\n\nTo better understand the impact of assuming (or simulating) a different latent scale we fit \\(k - 1\\) binomial regressions and check the estimated coefficients. We are not simulating a specific beta for each outcome but simulating a scale effect is actually impacting the regression coefficients. When generating data for a binary outcome the linear predictor is composed by \\(\\eta = \\beta_0 + \\beta_1x\\). The threshold \\(\\alpha\\) and slope of the function can be estimated using \\(\\alpha = -\\frac{\\beta_0}{\\beta_1}\\) and the slope is \\(\\frac{1}{\\beta_1}\\) (Faraggi et al., 2003; Knoblauch & Maloney, 2012). Under the proportional odds assumption, there is only a change in thresholds \\(\\alpha\\) this a shift in the sigmoid along the \\(x\\) axis. When including a scale effect a change in the sigmoid is combined with a change in the slope.\n\nset.seed(2023)\nk &lt;- 4\nn &lt;- 1e5\nb1 &lt;- 3\nd1 &lt;- log(2)\ndat &lt;- data.frame(x = runif(n))\ndat &lt;- sim_ord_latent(~x, ~x, beta = b1, zeta = d1, prob0 = rep(1/k, k), data = dat, link = \"probit\")\n\ndat$y1vs234 &lt;- ifelse(dat$y &lt;= 1, 1, 0)\ndat$y12vs34 &lt;- ifelse(dat$y &lt;= 2, 1, 0)\ndat$y123vs4 &lt;- ifelse(dat$y &lt;= 3, 1, 0)\n\ndat$y &lt;- ordered(dat$y)\nfit &lt;- clm(y ~ x, scale = ~x, data = dat, link = \"probit\")\n\nfit1vs234 &lt;- glm(y1vs234 ~ x, data = dat, family = binomial(link = \"probit\"))\nfit12vs34 &lt;- glm(y12vs34 ~ x, data = dat, family = binomial(link = \"probit\"))\nfit123vs4 &lt;- glm(y123vs4 ~ x, data = dat, family = binomial(link = \"probit\"))\n\nfits &lt;- list(y1vs234 = fit1vs234, fit12vs34 = fit12vs34, fit123vs4 = fit123vs4)\n\nlapply(fits, function(x) coef(x)) |&gt; \n  bind_rows(.id = \"model\") |&gt; \n  mutate(xp = list(seq(0, 5, 0.01))) |&gt; \n  unnest(xp) |&gt; \n  ggplot(aes(x = xp, y = `(Intercept)` + x * xp)) +\n  geom_line(aes(color = model)) +\n  theme_minimal(15)\n\n\n\n\n\n\n\n\n\n\n\nWhen fitting a model with clm() the way of relaxing the proportional odds or parallel slopes assumption is including what is called a nominal effect. Basically when a predictor is included as a nominal effect, there will be \\(k - 1\\) coefficients (where \\(k\\) is the number of ordered levels). In this way, the odds ratio or the difference in \\(z\\) scores will not be the same across \\(k\\) levels.\nIf a scale effect is included in clm() is not immediately clear what happens to the proportional odds assumption. In fact, we are still estimating a single \\(\\beta\\) but we have extra parameters on the scale of the distribution. However, changing the scale of the distribution is the same as changing the slope. This is clear from Figure 3 where changing the scale of the latent distribution is the same as changing the slope of the cumulative probability function. The slope (\\(\\beta_1\\)) is related to the scale of the distribution (logistic or normal) as \\(\\beta_1 = \\frac{1}{s}\\). In fact, if we include a scale effect we are changing the scale of the underlying latent distribution and thus the slope of of the functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn fact, if we simulate a model with a scale effect, odds ratios or \\(z\\) differences are no longer constant.\nWe will simulate the following situation where a binary predictor \\(x\\) has an effect both on location and scale of the logistic distribution.\n\nodds &lt;- function(p) p / (1 - p)\n\nn &lt;- 1e4 # large N\nk &lt;- 4\nb1 &lt;- log(3)\nz1 &lt;- log(2)\ndat &lt;- data.frame(x = rep(c(0, 1), each = n))\ndat &lt;- sim_ord_latent(~x, ~x, beta = log(3), zeta = log(2), prob0 = rep(1/k, k), data = dat, link = \"logit\")\n\n# model with scale effects, log(odds ratios) are different\n\nfit &lt;- clm(y ~ x, scale = ~ x, data = dat, link = \"logit\")\n\npreds &lt;- predict(fit, newdata = data.frame(x = c(0, 1)))$fit\npcum &lt;- t(apply(preds, 1, cumsum))[, 1:(k - 1)]\n\napply(pcum, 2, function(c) log(odds(c[1])/odds(c[2])))\n\n         1          2          3 \n-0.0436072  0.5202012  1.0755903 \n\n# model without scale effects, log(odds ratios) are equal\n\nfit0 &lt;- clm(y ~ x, scale = ~ 1, data = dat, link = \"logit\")\npreds &lt;- predict(fit0, newdata = data.frame(x = c(0, 1)))$fit\npcum &lt;- t(apply(preds, 1, cumsum))[, 1:(k - 1)]\napply(pcum, 2, function(c) log(odds(c[1])/odds(c[2])))\n\n        1         2         3 \n0.6107211 0.6107211 0.6107211 \n\n\n\ncat_latent_plot(c(0, 0 + b1), s = c(exp(0), (exp(0) + z1)), prob0 = rep(1/k, k), link = \"logit\")\n\n\n\n\n\n\n\n\nActually we can model this dataset using a non-proportional odds model. The model correctly capture the more complex probability structure but we are not able to recover the parameters because we simulated a scale effect.\n\nfit_nominal &lt;- clm(y ~ 1, nominal = ~ x, data = dat, link = \"logit\")\nsummary(fit_nominal)\n\nformula: y ~ 1\nnominal: ~x\ndata:    dat\n\n link  threshold nobs  logLik    AIC      niter max.grad cond.H \n logit flexible  20000 -26016.82 52045.65 8(0)  3.82e-12 7.8e+01\n\nThreshold coefficients:\n                Estimate Std. Error z value\n1|2.(Intercept) -1.12600    0.02325 -48.421\n2|3.(Intercept) -0.01120    0.02000  -0.560\n3|4.(Intercept)  1.09063    0.02305  47.320\n1|2.x            0.04227    0.03271   1.292\n2|3.x           -0.51759    0.02879 -17.981\n3|4.x           -1.07663    0.03052 -35.281\n\n\nEssentially we estimated two sets of thresholds one for the group 0 and one for the group 1. The difference with the previous model (beyond the fact that data are generated under a scale-location model) is that we need more parameters in a non-proportional odds model compared to the scale-location model. This is also more relevant when including more predictors. Also the likelihood ratio test correctly suggest that there is no relevant difference in the likelihood of the two models.\n\nanova(fit, fit_nominal)\n\nLikelihood ratio tests of cumulative link models:\n \n            formula: nominal: scale: link: threshold:\nfit         y ~ x    ~1       ~x     logit flexible  \nfit_nominal y ~ 1    ~x       ~1     logit flexible  \n\n            no.par   AIC logLik LR.stat df Pr(&gt;Chisq)\nfit              5 52044 -26017                      \nfit_nominal      6 52046 -26017  0.0478  1     0.8269"
  },
  {
    "objectID": "supplementary/location-scale-models.html#scale-effects",
    "href": "supplementary/location-scale-models.html#scale-effects",
    "title": "Location-Scale Ordinal Models",
    "section": "",
    "text": "The default ordinal regression model assume that the variance of the underlying latent distribution is the same across conditions. This is similar to a standard linear regression assuming the homogeneity of variances.\nFor example, when comparing two groups or conditions we can run a standard linear model (i.e., a t-test) assuming homogeneity of variances or using the Welch t-test (see Delacre et al., 2017) that relaxes the assumption. In addition, there are the so-called location-scale models that allows to include predictors also for the scale (e.g., the variance) of the distribution.\nThis can be done also in ordinal regression where instead of assuming the same variance between conditions, the linear predictors can be included. The Equations 1 and 2 expand the standard model including the linear predictor on the scale of the latent distribution.\n\\[\nP(Y \\leq k) = g^{-1}\\left(\\frac{\\alpha_k - \\mathbf{X}\\boldsymbol{\\beta}}{e^{\\boldsymbol{Z}\\boldsymbol{\\zeta}}}\\right)\n\\tag{1}\\]\n\\[\nY^\\star = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\;\\;\\; \\epsilon_i \\sim \\mathcal{N}(0, e^{Z\\zeta})\n\\tag{2}\\]\nWhere \\(X\\zeta\\) is the linear predictor for the scale of the distribution. By default for both the logit and probit model the scale is fixed to 1. On scale-location models we put predictors on both parameters. Given that the scale cannot be negative we use a log link function \\(\\eta = \\text{log}(X\\zeta)\\). As suggested by Tutz & Berger (2017) location-scale models can be considered as a more parsimonious approach compared to partially or completely relaxing the proportional odds assumption. Allowing the scale to be different as a function of the predictor create more modelling flexibility. Furthermore, two groups could be theoretically different only in the scale of the latent distribution with a similar location. In this example, the only way to capture group differences is by including a scale effect. The Figure 1 depict the impact of having different scales between two groups on the ordinal probabilities but the same location. Clearly, there is no location effect and the two groups are predicted to be the same considering only the location effect.\n\ncat_latent_plot(location = c(0, 0), scale = c(1, 2), prob0 = rep(1/4, 4), link = \"logit\", plot = FALSE) |&gt; \n  cowplot::plot_grid(plotlist = _, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nTutz (2022) provide a very clear and intuitive explanation of what happen when including a scale effect and how to interpret the result. As suggested before, the scale-location model allow to independently predict changes in the location and the scale. While location shifts are simply interpreted as increasing/decreasing the latent \\(\\mu\\) or the odds of responding a certain category scale effects are not straightforward. As the scale increase (e.g., the variance increase) there is an higher probability mass on extreme categories. On the other side as the scale decrease, responses are more concentrated on specific categories.\nThe location parameter determine the category and the scale determine the concentration around the category. For example, if one group have a certain latent mean \\(\\mu_1\\) and a small scale \\(\\sigma^2 = 1/3\\) (thus one third compared to the standard version of the distribution), all responses will be focused on categories around the latent mean. On the other side, increasing the scale will increase the cumulative probabilities for all categories and for values that tends to infinity extreme categories are preferred. The scale parameter can somehow be interpreted as the response style.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe location-scale model can be simulated using the sim_ord_latent() function and providing the predictors for the scale parameters. Given the log link function, predictors are provided on the log scale. For example, we simulate the effect of a binary variable \\(x\\) representing two independent groups predicting the \\(k = 5\\) response. We simulate a location effect of \\(\\beta_1 = 0.5\\) (in probit scale) and \\(\\zeta_1 = \\text{log}(2) = 0.70\\). The first group has a \\(\\sigma = 1\\) and the second group has \\(\\sigma = 2\\). Again we simulate that the baseline probabilities are uniform for the first group. The \\(\\log\\) link function (and \\(e\\) as inverse link function) is used to make sure that the variance is always positive. If the first group has a scale of 1 and the second group a scale of 2, the \\(\\zeta_1 = \\log(\\frac{s_2}{s_1}) = \\log(\\frac{2}{1}) \\approx 0.7\\). Thus the two groups have a log difference in terms of scale of 0.7 (or a ratio of 2).\n\nk &lt;- 5  # number of options\nn &lt;- 1e5 # number of observations\nb1 &lt;- 0.5 # beta1, the shift in the latent distribution\nz1 &lt;- log(2) # zeta1, the change in the scale\nprobs0 &lt;- rep(1/k, k) # probabilities when x = 0\nalphas &lt;- prob_to_alpha(probs0, link = \"probit\") # get true thresholds from probabilities\ndat &lt;- data.frame(x = rep(c(0, 1), each = n/2))\ndat &lt;- sim_ord_latent(~x, scale = ~x, beta = b1, zeta = z1, prob0 = probs0, data = dat, link = \"probit\")\nfit &lt;- clm(y ~ x, scale = ~ x, data = dat, link = \"probit\")\nsummary(fit)\n\nformula: y ~ x\nscale:   ~x\ndata:    dat\n\n link   threshold nobs  logLik     AIC       niter max.grad cond.H \n probit flexible  1e+05 -151495.53 303003.05 11(0) 4.16e-07 3.1e+01\n\nCoefficients:\n  Estimate Std. Error z value Pr(&gt;|z|)    \nx  0.48797    0.01165   41.88   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlog-scale coefficients:\n  Estimate Std. Error z value Pr(&gt;|z|)    \nx 0.694460   0.008343   83.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n     Estimate Std. Error z value\n1|2 -0.849586   0.006316  -134.5\n2|3 -0.258767   0.005425   -47.7\n3|4  0.247654   0.005407    45.8\n4|5  0.832103   0.006281   132.5\n\n\nTo better understand the impact of assuming (or simulating) a different latent scale we fit \\(k - 1\\) binomial regressions and check the estimated coefficients. We are not simulating a specific beta for each outcome but simulating a scale effect is actually impacting the regression coefficients. When generating data for a binary outcome the linear predictor is composed by \\(\\eta = \\beta_0 + \\beta_1x\\). The threshold \\(\\alpha\\) and slope of the function can be estimated using \\(\\alpha = -\\frac{\\beta_0}{\\beta_1}\\) and the slope is \\(\\frac{1}{\\beta_1}\\) (Faraggi et al., 2003; Knoblauch & Maloney, 2012). Under the proportional odds assumption, there is only a change in thresholds \\(\\alpha\\) this a shift in the sigmoid along the \\(x\\) axis. When including a scale effect a change in the sigmoid is combined with a change in the slope.\n\nset.seed(2023)\nk &lt;- 4\nn &lt;- 1e5\nb1 &lt;- 3\nd1 &lt;- log(2)\ndat &lt;- data.frame(x = runif(n))\ndat &lt;- sim_ord_latent(~x, ~x, beta = b1, zeta = d1, prob0 = rep(1/k, k), data = dat, link = \"probit\")\n\ndat$y1vs234 &lt;- ifelse(dat$y &lt;= 1, 1, 0)\ndat$y12vs34 &lt;- ifelse(dat$y &lt;= 2, 1, 0)\ndat$y123vs4 &lt;- ifelse(dat$y &lt;= 3, 1, 0)\n\ndat$y &lt;- ordered(dat$y)\nfit &lt;- clm(y ~ x, scale = ~x, data = dat, link = \"probit\")\n\nfit1vs234 &lt;- glm(y1vs234 ~ x, data = dat, family = binomial(link = \"probit\"))\nfit12vs34 &lt;- glm(y12vs34 ~ x, data = dat, family = binomial(link = \"probit\"))\nfit123vs4 &lt;- glm(y123vs4 ~ x, data = dat, family = binomial(link = \"probit\"))\n\nfits &lt;- list(y1vs234 = fit1vs234, fit12vs34 = fit12vs34, fit123vs4 = fit123vs4)\n\nlapply(fits, function(x) coef(x)) |&gt; \n  bind_rows(.id = \"model\") |&gt; \n  mutate(xp = list(seq(0, 5, 0.01))) |&gt; \n  unnest(xp) |&gt; \n  ggplot(aes(x = xp, y = `(Intercept)` + x * xp)) +\n  geom_line(aes(color = model)) +\n  theme_minimal(15)\n\n\n\n\n\n\n\n\n\n\n\nWhen fitting a model with clm() the way of relaxing the proportional odds or parallel slopes assumption is including what is called a nominal effect. Basically when a predictor is included as a nominal effect, there will be \\(k - 1\\) coefficients (where \\(k\\) is the number of ordered levels). In this way, the odds ratio or the difference in \\(z\\) scores will not be the same across \\(k\\) levels.\nIf a scale effect is included in clm() is not immediately clear what happens to the proportional odds assumption. In fact, we are still estimating a single \\(\\beta\\) but we have extra parameters on the scale of the distribution. However, changing the scale of the distribution is the same as changing the slope. This is clear from Figure 3 where changing the scale of the latent distribution is the same as changing the slope of the cumulative probability function. The slope (\\(\\beta_1\\)) is related to the scale of the distribution (logistic or normal) as \\(\\beta_1 = \\frac{1}{s}\\). In fact, if we include a scale effect we are changing the scale of the underlying latent distribution and thus the slope of of the functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn fact, if we simulate a model with a scale effect, odds ratios or \\(z\\) differences are no longer constant.\nWe will simulate the following situation where a binary predictor \\(x\\) has an effect both on location and scale of the logistic distribution.\n\nodds &lt;- function(p) p / (1 - p)\n\nn &lt;- 1e4 # large N\nk &lt;- 4\nb1 &lt;- log(3)\nz1 &lt;- log(2)\ndat &lt;- data.frame(x = rep(c(0, 1), each = n))\ndat &lt;- sim_ord_latent(~x, ~x, beta = log(3), zeta = log(2), prob0 = rep(1/k, k), data = dat, link = \"logit\")\n\n# model with scale effects, log(odds ratios) are different\n\nfit &lt;- clm(y ~ x, scale = ~ x, data = dat, link = \"logit\")\n\npreds &lt;- predict(fit, newdata = data.frame(x = c(0, 1)))$fit\npcum &lt;- t(apply(preds, 1, cumsum))[, 1:(k - 1)]\n\napply(pcum, 2, function(c) log(odds(c[1])/odds(c[2])))\n\n         1          2          3 \n-0.0436072  0.5202012  1.0755903 \n\n# model without scale effects, log(odds ratios) are equal\n\nfit0 &lt;- clm(y ~ x, scale = ~ 1, data = dat, link = \"logit\")\npreds &lt;- predict(fit0, newdata = data.frame(x = c(0, 1)))$fit\npcum &lt;- t(apply(preds, 1, cumsum))[, 1:(k - 1)]\napply(pcum, 2, function(c) log(odds(c[1])/odds(c[2])))\n\n        1         2         3 \n0.6107211 0.6107211 0.6107211 \n\n\n\ncat_latent_plot(c(0, 0 + b1), s = c(exp(0), (exp(0) + z1)), prob0 = rep(1/k, k), link = \"logit\")\n\n\n\n\n\n\n\n\nActually we can model this dataset using a non-proportional odds model. The model correctly capture the more complex probability structure but we are not able to recover the parameters because we simulated a scale effect.\n\nfit_nominal &lt;- clm(y ~ 1, nominal = ~ x, data = dat, link = \"logit\")\nsummary(fit_nominal)\n\nformula: y ~ 1\nnominal: ~x\ndata:    dat\n\n link  threshold nobs  logLik    AIC      niter max.grad cond.H \n logit flexible  20000 -26016.82 52045.65 8(0)  3.82e-12 7.8e+01\n\nThreshold coefficients:\n                Estimate Std. Error z value\n1|2.(Intercept) -1.12600    0.02325 -48.421\n2|3.(Intercept) -0.01120    0.02000  -0.560\n3|4.(Intercept)  1.09063    0.02305  47.320\n1|2.x            0.04227    0.03271   1.292\n2|3.x           -0.51759    0.02879 -17.981\n3|4.x           -1.07663    0.03052 -35.281\n\n\nEssentially we estimated two sets of thresholds one for the group 0 and one for the group 1. The difference with the previous model (beyond the fact that data are generated under a scale-location model) is that we need more parameters in a non-proportional odds model compared to the scale-location model. This is also more relevant when including more predictors. Also the likelihood ratio test correctly suggest that there is no relevant difference in the likelihood of the two models.\n\nanova(fit, fit_nominal)\n\nLikelihood ratio tests of cumulative link models:\n \n            formula: nominal: scale: link: threshold:\nfit         y ~ x    ~1       ~x     logit flexible  \nfit_nominal y ~ 1    ~x       ~1     logit flexible  \n\n            no.par   AIC logLik LR.stat df Pr(&gt;Chisq)\nfit              5 52044 -26017                      \nfit_nominal      6 52046 -26017  0.0478  1     0.8269"
  },
  {
    "objectID": "supplementary/location-scale-models.html#location-shift-models",
    "href": "supplementary/location-scale-models.html#location-shift-models",
    "title": "Location-Scale Ordinal Models",
    "section": "Location Shift Models",
    "text": "Location Shift Models\nThe location-shift model is considered an alternative way to include dispersion effects (together with location effects) (Tutz, 2022; Tutz & Berger, 2017; Tutz & Berger, 2020). The core of the models is including predictors on the thresholds \\(\\alpha\\) to decrease/increase the probability of response categories \\(k\\) together with the parameters on the location \\(\\boldsymbol{\\beta}\\)\nThey are similar to the location-scale models in terms of parsimony but different in terms of parameters interpretation. Given that these models cannot be directly fitted by the ordinal package we are not going into details but we included some references to understand the model parametrization. The models are implemented in the ordDisp R package (https://cran.r-project.org/web/packages/ordDisp/index.html).\nHowever, as noted by Berger and Tutz1 there is a correspondence between the partial proportional odds model and the location-shift model. Let’s simulate a model with a scale effect where the true model has a scale effect:\n\nlibrary(ordinal)\nlibrary(ordDisp)\n\nn &lt;- 1e3\nk &lt;- 3 # number of ordinal levels\nprobs0 &lt;- rep(1/k, k) # baseline probabilities\n\ndat &lt;- data.frame(x = rep(c(0, 1), each = n))\n\ndat &lt;- sim_ord_latent(~x, ~x, \n                      beta = b1, zeta = z1, \n                      prob0 = probs0, \n                      link = \"logit\", \n                      data = dat, \n                      simulate = TRUE)\n\nhead(dat)\n\n  x y          ys\n1 0 1 -2.51596242\n2 0 3  4.14198274\n3 0 2 -0.06790548\n4 0 2  0.43393823\n5 0 1 -1.86065820\n6 0 3  0.87618582\n\n\nNow let’s fit the location-shift model and the non-proportional odds model:\n\nfit_shift &lt;- ordDisp(y ~ x|x, data = dat)\nfit_nopo &lt;- clm(y ~ 1, nominal = ~x, data = dat, link = \"logit\")\n\nsummary(fit_shift)\n## \n## Call:\n## vglm(formula = formula1, family = cumulative(parallel = FALSE ~ \n##     1, reverse = reverse), data = DM, form2 = formula2, xij = formula3, \n##     checkwz = FALSE)\n## \n## Coefficients: \n##               Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept):1 -0.71724    0.06736 -10.649  &lt; 2e-16 ***\n## (Intercept):2  0.65884    0.06671   9.876  &lt; 2e-16 ***\n## xx            -0.43974    0.08410  -5.229 1.71e-07 ***\n## xz            -0.63838    0.08481  -7.527 5.18e-14 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Names of linear predictors: logitlink(P[Y&lt;=1]), logitlink(P[Y&lt;=2])\n## \n## Residual deviance: 4203.753 on 3996 degrees of freedom\n## \n## Log-likelihood: -2101.876 on 3996 degrees of freedom\n## \n## Number of Fisher scoring iterations: 3 \n## \n## No Hauck-Donner effect found in any of the estimates\n## \n## \n## Exponentiated coefficients:\n##        xx        xz \n## 0.6442065 0.5281488\nsummary(fit_nopo)\n## formula: y ~ 1\n## nominal: ~x\n## data:    dat\n## \n##  link  threshold nobs logLik   AIC     niter max.grad cond.H \n##  logit flexible  2000 -2101.88 4211.75 6(0)  9.75e-08 3.2e+01\n## \n## Threshold coefficients:\n##                 Estimate Std. Error z value\n## 1|2.(Intercept) -0.71724    0.06736 -10.649\n## 2|3.(Intercept)  0.65884    0.06671   9.876\n## 1|2.x           -0.12055    0.09634  -1.251\n## 2|3.x           -0.75892    0.09198  -8.251\n\nFirstly, there is a correspondence in the baseline thresholds:\n\n# location-shift model\ncoef(fit_shift)[1:(k - 1)]\n## (Intercept):1 (Intercept):2 \n##    -0.7172447     0.6588411\n\n# non proportional odds model\ncoef(fit_nopo)[1:(k - 1)]\n## 1|2.(Intercept) 2|3.(Intercept) \n##      -0.7172447       0.6588411\n\nThen also linear predictors \\(\\eta\\) are the same:\n\n# let's see the linear predictor for x = 1\n\n# location-shift model\npredict(fit_shift)[n + 1, ]\n## logitlink(P[Y&lt;=1]) logitlink(P[Y&lt;=2]) \n##         -0.8377921         -0.1000835\n\n# non proportional odds model\npredict(fit_nopo, newdata = data.frame(x = 1), type = \"linear.predictor\")$eta1[, -k]\n##          1          2 \n## -0.8377921 -0.1000835"
  },
  {
    "objectID": "supplementary/location-scale-models.html#footnotes",
    "href": "supplementary/location-scale-models.html#footnotes",
    "title": "Location-Scale Ordinal Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.researchgate.net/profile/Gerhard-Tutz/publication/305082385_Modelling_of_Varying_Dispersion_in_Cumulative_Regression_Models/links/578115fd08ae9485a43bd0f0/Modelling-of-Varying-Dispersion-in-Cumulative-Regression-Models.pdf↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simulating ordinal data",
    "section": "",
    "text": "This is the online supplementary material for the tutorial-paper Ordinal regression models made easy. A tutorial on parameter interpretation, data simulation, and power analysis.\nIn the next links there are useful materials and examples that extended what presented in the tutorial-paper.\n\nUseful resources: a collection of books, blog posts, R packages and more about ordinal data\nOrdinal notes: a miscellanea of code snippets and quick examples about ordinal regression models\nLocation-scale models: a tutorial to fit, interpret and simulate location-scale cumulative link models.\nSimulating a GLM: a quick example on how to simulate data for a Generalized Linear Model (GLM).\nMixed-effects models: a quick example on how to simulate data for a Generalized Linear Model (GLM)."
  },
  {
    "objectID": "supplementary/useful-resources.html",
    "href": "supplementary/useful-resources.html",
    "title": "Useful Resources",
    "section": "",
    "text": "In this document we collect some useful resources for ordinal regression models.\n\nBooks\n\nAnalysis of Ordinal Categorical Data: a very complete book about modeling ordinal data.\nCategorical Data Analysis\nAn Introduction to Categorical Data Analysis: Introduction about categorical data in R with some examples with ordinal data (Chapter 6)\nDoing Bayesian Data Analysis: Overview of Bayesian data analysis. Chapter 23 is about ordinal regression models.\nRegression and Other Stories: Chapter 15 contains some examples and parametrizations of ordinal regression models\n\n\n\nPosts\n\nViolation of Proportional Odds is Not Fatal\nAssessing the Proportional Odds Assumption and Its Impact\nResources for Ordinal Regression Models\nBayesian Estimation of Signal Detection Models\nCausal inference with ordinal regression\n\n\n\nPackages\n\nordinal: https://cran.r-project.org/web/packages/ordinal/index.html\nVGAM: https://cran.r-project.org/web/packages/VGAM/index.html\nbrms: https://paul-buerkner.github.io/brms/\nMASS::polr(): https://rdrr.io/cran/MASS/man/polr.html\nordDisp: https://cran.r-project.org/web/packages/ordDisp/index.html"
  },
  {
    "objectID": "supplementary/ordinal-notes.html",
    "href": "supplementary/ordinal-notes.html",
    "title": "Ordinal Notes",
    "section": "",
    "text": "In the tutorial we used the \\(\\alpha_k -\\mathbf{X}\\boldsymbol{\\beta}\\) parametrization (thus with the minus sign) because this force the \\(\\beta\\) to have the interpretation as in standard regression models. Usually, \\(\\beta\\) is the increase in \\(y\\) for a unit increase in \\(x\\). A negative \\(\\beta\\) means that the expected value of \\(y\\) decrease for an increase in \\(x\\). Let’s see an example using the positive sign for \\(\\beta\\).\n\nk &lt;- 4 # number of ordinal outcomes\nx &lt;- c(0, 1) # a binary predictor\nb1 &lt;- log(3) # log odds ratio comparing x1 and x0\nprobs0 &lt;- rep(1/k, k)\nalpha &lt;- prob_to_alpha(probs0, link = \"logit\")\nX &lt;- matrix(x, nrow = 2)\n\n# positive sign\n(lp &lt;- lapply(alpha, function(a) c(a + X %*% b1)))\n\n$`1|2`\n[1] -1.098612  0.000000\n\n$`2|3`\n[1] 0.000000 1.098612\n\n$`3|4`\n[1] 1.098612 2.197225\n\ncump &lt;- lapply(lp, plogis)\ncump &lt;- cbind(0, data.frame(cump), 1)\np &lt;- data.frame(t(apply(cump, 1, diff)))\nnames(p) &lt;- paste0(\"y\", 1:k)\np$x &lt;- x\np\n\n    y1   y2   y3   y4 x\n1 0.25 0.25 0.25 0.25 0\n2 0.50 0.25 0.15 0.10 1\n\np |&gt; \n  pivot_longer(starts_with(\"y\")) |&gt; \n  ggplot(aes(x = x, y = value, fill = name)) +\n  geom_col(position = position_dodge()) +\n  ylab(\"Probability\") +\n  ylim(c(0, 1)) +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") +\n  ggtitle(latex2exp::TeX(\"$P(Y \\\\leq k) = \\\\alpha_k + X\\\\beta$\"))\n\n\n\n\n\n\n\n\nClearly a positive \\(\\beta\\) create higher probability for lower \\(Y\\) categories. This can be somehow not intuitive thus we can use the negative sign.\n\n# negative sign\n\n(lp &lt;- lapply(alpha, function(a) c(a - X %*% b1)))\n\n$`1|2`\n[1] -1.098612 -2.197225\n\n$`2|3`\n[1]  0.000000 -1.098612\n\n$`3|4`\n[1] 1.098612 0.000000\n\ncump &lt;- lapply(lp, plogis)\ncump &lt;- cbind(0, data.frame(cump), 1)\np &lt;- data.frame(t(apply(cump, 1, diff)))\nnames(p) &lt;- paste0(\"y\", 1:k)\np$x &lt;- x\n\np |&gt; \n  pivot_longer(starts_with(\"y\")) |&gt; \n  ggplot(aes(x = x, y = value, fill = name)) +\n  geom_col(position = position_dodge()) +\n  ylab(\"Probability\") +\n  ylim(c(0, 1)) +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") +\n  ggtitle(latex2exp::TeX(\"$P(Y \\\\leq k) = \\\\alpha_k - X\\\\beta$\"))\n\n\n\n\n\n\n\n\nUsing the second parametrization, with positive \\(\\beta\\) we have higher probability for higher \\(Y\\) categories and the opposite.\nThe negative-sign parametrization is implicit when simulating from the latent distribution. Let’s see an example. We use a continuous \\(x\\) predictor because it is easier to see the results.\n\nn &lt;- 1e3\nx &lt;- runif(1e3)\nB &lt;- 3\nys_p &lt;- x * B + rnorm(n)\nys_n &lt;- x * -B + rnorm(n)\ny_p &lt;- findInterval(ys_p, alpha) + 1\ny_n &lt;- findInterval(ys_n, alpha) + 1\n\npar(mfrow = c(1,2))\nplot(x, ys_p, col = y_p, pch = 19, ylim = c(-7, 7), main = latex2exp::TeX(\"$\\\\Y^{*} = X \\\\beta + \\\\epsilon$, $\\\\beta = 3$\"), ylab = latex2exp::TeX(\"$Y^{*}$\"))\nabline(h = alpha, lty = \"dashed\")\nlegend(\"bottomleft\", fill = 1:k, legend = paste0(\"Y\", 1:k))\n\nplot(x, ys_n, col = y_n, pch = 19, ylim = c(-7, 7), main = latex2exp::TeX(\"$\\\\Y^{*} = X \\\\beta + \\\\epsilon$, $\\\\beta = -3$\"), ylab = latex2exp::TeX(\"$Y^{*}$\"))\nabline(h = alpha, lty = \"dashed\")\n\n\n\n\nSimulated data using the latent variable approach. The dotted lines are the thresholds \\(\\alpha\\)"
  },
  {
    "objectID": "supplementary/ordinal-notes.html#mathbfxboldsymbolbeta-vs-mathbfxboldsymbolbeta-parametrization",
    "href": "supplementary/ordinal-notes.html#mathbfxboldsymbolbeta-vs-mathbfxboldsymbolbeta-parametrization",
    "title": "Ordinal Notes",
    "section": "",
    "text": "In the tutorial we used the \\(\\alpha_k -\\mathbf{X}\\boldsymbol{\\beta}\\) parametrization (thus with the minus sign) because this force the \\(\\beta\\) to have the interpretation as in standard regression models. Usually, \\(\\beta\\) is the increase in \\(y\\) for a unit increase in \\(x\\). A negative \\(\\beta\\) means that the expected value of \\(y\\) decrease for an increase in \\(x\\). Let’s see an example using the positive sign for \\(\\beta\\).\n\nk &lt;- 4 # number of ordinal outcomes\nx &lt;- c(0, 1) # a binary predictor\nb1 &lt;- log(3) # log odds ratio comparing x1 and x0\nprobs0 &lt;- rep(1/k, k)\nalpha &lt;- prob_to_alpha(probs0, link = \"logit\")\nX &lt;- matrix(x, nrow = 2)\n\n# positive sign\n(lp &lt;- lapply(alpha, function(a) c(a + X %*% b1)))\n\n$`1|2`\n[1] -1.098612  0.000000\n\n$`2|3`\n[1] 0.000000 1.098612\n\n$`3|4`\n[1] 1.098612 2.197225\n\ncump &lt;- lapply(lp, plogis)\ncump &lt;- cbind(0, data.frame(cump), 1)\np &lt;- data.frame(t(apply(cump, 1, diff)))\nnames(p) &lt;- paste0(\"y\", 1:k)\np$x &lt;- x\np\n\n    y1   y2   y3   y4 x\n1 0.25 0.25 0.25 0.25 0\n2 0.50 0.25 0.15 0.10 1\n\np |&gt; \n  pivot_longer(starts_with(\"y\")) |&gt; \n  ggplot(aes(x = x, y = value, fill = name)) +\n  geom_col(position = position_dodge()) +\n  ylab(\"Probability\") +\n  ylim(c(0, 1)) +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") +\n  ggtitle(latex2exp::TeX(\"$P(Y \\\\leq k) = \\\\alpha_k + X\\\\beta$\"))\n\n\n\n\n\n\n\n\nClearly a positive \\(\\beta\\) create higher probability for lower \\(Y\\) categories. This can be somehow not intuitive thus we can use the negative sign.\n\n# negative sign\n\n(lp &lt;- lapply(alpha, function(a) c(a - X %*% b1)))\n\n$`1|2`\n[1] -1.098612 -2.197225\n\n$`2|3`\n[1]  0.000000 -1.098612\n\n$`3|4`\n[1] 1.098612 0.000000\n\ncump &lt;- lapply(lp, plogis)\ncump &lt;- cbind(0, data.frame(cump), 1)\np &lt;- data.frame(t(apply(cump, 1, diff)))\nnames(p) &lt;- paste0(\"y\", 1:k)\np$x &lt;- x\n\np |&gt; \n  pivot_longer(starts_with(\"y\")) |&gt; \n  ggplot(aes(x = x, y = value, fill = name)) +\n  geom_col(position = position_dodge()) +\n  ylab(\"Probability\") +\n  ylim(c(0, 1)) +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") +\n  ggtitle(latex2exp::TeX(\"$P(Y \\\\leq k) = \\\\alpha_k - X\\\\beta$\"))\n\n\n\n\n\n\n\n\nUsing the second parametrization, with positive \\(\\beta\\) we have higher probability for higher \\(Y\\) categories and the opposite.\nThe negative-sign parametrization is implicit when simulating from the latent distribution. Let’s see an example. We use a continuous \\(x\\) predictor because it is easier to see the results.\n\nn &lt;- 1e3\nx &lt;- runif(1e3)\nB &lt;- 3\nys_p &lt;- x * B + rnorm(n)\nys_n &lt;- x * -B + rnorm(n)\ny_p &lt;- findInterval(ys_p, alpha) + 1\ny_n &lt;- findInterval(ys_n, alpha) + 1\n\npar(mfrow = c(1,2))\nplot(x, ys_p, col = y_p, pch = 19, ylim = c(-7, 7), main = latex2exp::TeX(\"$\\\\Y^{*} = X \\\\beta + \\\\epsilon$, $\\\\beta = 3$\"), ylab = latex2exp::TeX(\"$Y^{*}$\"))\nabline(h = alpha, lty = \"dashed\")\nlegend(\"bottomleft\", fill = 1:k, legend = paste0(\"Y\", 1:k))\n\nplot(x, ys_n, col = y_n, pch = 19, ylim = c(-7, 7), main = latex2exp::TeX(\"$\\\\Y^{*} = X \\\\beta + \\\\epsilon$, $\\\\beta = -3$\"), ylab = latex2exp::TeX(\"$Y^{*}$\"))\nabline(h = alpha, lty = \"dashed\")\n\n\n\n\nSimulated data using the latent variable approach. The dotted lines are the thresholds \\(\\alpha\\)"
  },
  {
    "objectID": "supplementary/ordinal-notes.html#checking-the-impact-of-mathbfbeta",
    "href": "supplementary/ordinal-notes.html#checking-the-impact-of-mathbfbeta",
    "title": "Ordinal Notes",
    "section": "Checking the impact of \\(\\mathbf{\\beta}\\)",
    "text": "Checking the impact of \\(\\mathbf{\\beta}\\)\nChoosing one or more plausible \\(\\beta_j\\) values can be challenging. For a single \\(\\beta\\) we can easily think about the odds ratio (for a logit model) or the Cohen’s \\(d\\) (for a probit) model. With multiple predictors and their interactions is not easy to fix plausible values. A good strategy is to try different values and check the impact on the predicted probabilities. In practice, we need to compute the predicted probabilities using the model equation for the \\(k\\) ordinal outcomes. This can be easily done with the sim_ord_latent() function, fixing the simulate = FALSE parameter. In this way, only the predicted probabilities are computed. Let’s see an example for a single \\(x\\) sampled for a standard normal distribution.\n\nk &lt;- 4\ndat &lt;- data.frame(x = seq(-4, 4, 0.1))\nb1 &lt;- 0.5\nprobs0 &lt;- rep(1/k, k)\ndat &lt;- sim_ord_latent(~x, beta = b1, prob0 = probs0, data = dat, simulate = FALSE)\nhead(dat)\n\n     x       yp1      yp12     yp123        y1        y2         y3         y4\n1 -4.0 0.7112346 0.8807971 0.9568355 0.7112346 0.1695625 0.07603839 0.04316453\n2 -3.9 0.7008582 0.8754466 0.9547226 0.7008582 0.1745885 0.07927594 0.04527742\n3 -3.8 0.6902712 0.8698915 0.9525114 0.6902712 0.1796203 0.08261987 0.04748860\n4 -3.7 0.6794810 0.8641271 0.9501979 0.6794810 0.1846461 0.08607076 0.04980214\n5 -3.6 0.6684954 0.8581489 0.9477778 0.6684954 0.1896536 0.08962886 0.05222221\n6 -3.5 0.6573231 0.8519528 0.9452469 0.6573231 0.1946297 0.09329410 0.05475309\n\n\nThen we can plot the results:\n\ndat |&gt; \n  pivot_longer(matches(\"^y[1-9]\")) |&gt; \n  ggplot(aes(x = x, y = value, color = name)) +\n  geom_line()\n\n\n\n\n\n\n\n\nIn this case the \\(\\beta_1 = 0.5\\) can be considered a plausible value. Let’s see what happens increasing it:\n\ndata.frame(x = seq(-4, 4, 0.1)) |&gt; \n  sim_ord_latent(~x, beta = 4, prob0 = probs0, data = _, simulate = FALSE) |&gt; \n  pivot_longer(matches(\"^y[1-9]\")) |&gt; \n  ggplot(aes(x = x, y = value, color = name)) +\n  geom_line()\n\n\n\n\n\n\n\n\nWe can clearly see the difference and probably \\(\\beta = 4\\) can be considered too large. To note, the same result can be achieved using the num_latent_plot() (that under the hood uses the sim_ord_latent() function).\nLet’s make now an example, with an interaction between a continous and categorical predictor.\n\ndat &lt;- expand_grid(x = seq(-4, 4, 0.1), g = c(\"a\", \"b\"))\ndat$g &lt;- factor(dat$g)\ncontrasts(dat$g) &lt;- c(-0.5, 0.5)\nbeta &lt;- c(b1 = 0.5, b2 = 1, b3 = 0.1)\ndat &lt;- sim_ord_latent(~ x * g, beta = beta, prob0 = probs0, data = dat, simulate = FALSE)\n\ndat |&gt; \n  pivot_longer(matches(\"^y[1-9]\")) |&gt; \n  ggplot(aes(x = x, y = value, color = name, lty = g)) +\n  geom_line()\n\n\n\n\n\n\n\n\nWe can see the impact of \\(\\beta_3 = 0.1\\) that is the difference in slopes between the two groups. We can also use another plot to better see the group effect on each \\(Y\\).\n\ndat |&gt; \n  pivot_longer(matches(\"^y[1-9]\")) |&gt; \n  ggplot(aes(x = x, y = value, color = g)) +\n  geom_line() +\n  facet_wrap(~name)"
  },
  {
    "objectID": "supplementary/ordinal-notes.html#checking-scale-effects",
    "href": "supplementary/ordinal-notes.html#checking-scale-effects",
    "title": "Ordinal Notes",
    "section": "Checking scale effects",
    "text": "Checking scale effects\nWith a numerical \\(x\\), checking the impact of scale effects is not easy (at least compared to the categorical case). For example, using the sim_ord_latent() function we can see the impact on the predicted probabilities of simulating a scale effect:\n\nx &lt;- runif(100)\nb1 &lt;- 10\nz1 &lt;- 2\nk &lt;- 4\n\ndat &lt;- data.frame(x = runif(100)) |&gt; \n  sim_ord_latent(~x, ~x, beta = b1, zeta = z1, prob0 = rep(1/k, k), data = _, simulate = FALSE, link = \"logit\")\nhead(dat)\n\n           x        yp1      yp12     yp123         y1         y2         y3\n1 0.08801661 0.15984470 0.3234201 0.5456691 0.15984470 0.16357537 0.22224900\n2 0.31533423 0.09423033 0.1573178 0.2509408 0.09423033 0.06308746 0.09362299\n3 0.19417205 0.11285650 0.2113455 0.3608266 0.11285650 0.09848903 0.14948109\n4 0.04250831 0.19789393 0.4036120 0.6499079 0.19789393 0.20571804 0.24629596\n5 0.49774324 0.09575014 0.1371248 0.1925705 0.09575014 0.04137468 0.05544571\n6 0.92380891 0.16394135 0.1890745 0.2170603 0.16394135 0.02513311 0.02798581\n         y4\n1 0.4543309\n2 0.7490592\n3 0.6391734\n4 0.3500921\n5 0.8074295\n6 0.7829397\n\ndat |&gt; \n  pivot_longer(matches(\"^y[1-9]\")) |&gt; \n  ggplot(aes(x = x, y = value, color = name)) +\n  geom_line()\n\n\n\n\n\n\n\n\nWe see only the expected probabilities but is not clear the impact of the scale effect. Instead we can simulate data and calculate the expected value of the location (\\(\\mu\\)) and scale (\\(s\\)) for a categorized version of \\(x\\). Then plotting the average location and scale we can understand the impact of choosing a specific parameter value with a numerical predictor.\n\ndata.frame(x = runif(1e5)) |&gt;\n  sim_ord_latent(~x, ~x, beta = b1, zeta = z1, prob0 = rep(1/k, k), data = _, simulate = TRUE, link = \"probit\") |&gt; \n  mutate(xc = cut(x, seq(0, 1, 0.1), include.lowest = TRUE),\n         xc = as.integer(xc)) |&gt; \n  group_by(xc) |&gt; \n  summarise(location = mean(ys),\n            scale = sd(ys)) |&gt; \n  pivot_longer(c(location, scale)) |&gt; \n  ggplot(aes(x = xc, y = value)) +\n  facet_wrap(~name) +\n  geom_line()"
  }
]